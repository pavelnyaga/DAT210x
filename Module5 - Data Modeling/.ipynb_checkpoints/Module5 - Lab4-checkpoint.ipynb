{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT210x - Programming with Python for DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module5- Lab4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "matplotlib.style.use('ggplot') # Look Pretty\n",
    "c = ['red', 'green', 'blue', 'orange', 'yellow', 'brown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can experiment with these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_TYPE_TEXT = False    # If you'd like to see indices\n",
    "PLOT_VECTORS = True       # If you'd like to see your original features in P.C.-Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Convenience Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawVectors(transformed_features, components_, columns, plt):\n",
    "    num_columns = len(columns)\n",
    "\n",
    "    # This function will project your *original* feature (columns)\n",
    "    # onto your principal component feature-space, so that you can\n",
    "    # visualize how \"important\" each one was in the\n",
    "    # multi-dimensional scaling\n",
    "\n",
    "    # Scale the principal components by the max value in\n",
    "    # the transformed set belonging to that component\n",
    "    xvector = components_[0] * max(transformed_features[:,0])\n",
    "    yvector = components_[1] * max(transformed_features[:,1])\n",
    "\n",
    "    ## Visualize projections\n",
    "\n",
    "    # Sort each column by its length. These are your *original*\n",
    "    # columns, not the principal components.\n",
    "    important_features = { columns[i] : math.sqrt(xvector[i]**2 + yvector[i]**2) for i in range(num_columns) }\n",
    "    important_features = sorted(zip(important_features.values(), important_features.keys()), reverse=True)\n",
    "    print(\"Projected Features by importance:\\n\", important_features)\n",
    "\n",
    "    ax = plt.axes()\n",
    "\n",
    "    for i in range(num_columns):\n",
    "        # Use an arrow to project each original feature as a\n",
    "        # labeled vector on your principal component axes\n",
    "        plt.arrow(0, 0, xvector[i], yvector[i], color='b', width=0.0005, head_width=0.02, alpha=0.75, zorder=600000)\n",
    "        plt.text(xvector[i]*1.2, yvector[i]*1.2, list(columns)[i], color='b', alpha=0.75, zorder=600000)\n",
    "        \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doPCA(data, dimensions=2):\n",
    "    model = PCA(n_components=dimensions, svd_solver='randomized', random_state=7)\n",
    "    model.fit(data)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doKMeans(data, num_clusters=0):\n",
    "    # TODO: Do the KMeans clustering here, passing in the # of clusters parameter\n",
    "    # and fit it against your data. Then, return a tuple containing the cluster\n",
    "    # centers and the labels.\n",
    "    #\n",
    "    # Hint: Just like with doPCA above, you will have to create a variable called\n",
    "    # `model`, which will be a SKLearn K-Means model for this to work.\n",
    "    \n",
    "    \n",
    "    model = KMeans(n_clusters = num_clusters)\n",
    "    labels = model.fit_predict(data)\n",
    "    \n",
    "    return model.cluster_centers_, model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Load up the dataset. It may or may not have nans in it. Make sure you catch them and destroy them, by setting them to `0`. This is valid for this dataset, since if the value is missing, you can assume no money was spent on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Channel  Region  Fresh   Milk  Grocery  Frozen  Detergents_Paper  \\\n",
      "0          2       3  12669   9656     7561     214              2674   \n",
      "1          2       3   7057   9810     9568    1762              3293   \n",
      "2          2       3   6353   8808     7684    2405              3516   \n",
      "3          1       3  13265   1196     4221    6404               507   \n",
      "4          2       3  22615   5410     7198    3915              1777   \n",
      "5          2       3   9413   8259     5126     666              1795   \n",
      "6          2       3  12126   3199     6975     480              3140   \n",
      "7          2       3   7579   4956     9426    1669              3321   \n",
      "8          1       3   5963   3648     6192     425              1716   \n",
      "9          2       3   6006  11093    18881    1159              7425   \n",
      "10         2       3   3366   5403    12974    4400              5977   \n",
      "11         2       3  13146   1124     4523    1420               549   \n",
      "12         2       3  31714  12319    11757     287              3881   \n",
      "13         2       3  21217   6208    14982    3095              6707   \n",
      "14         2       3  24653   9465    12091     294              5058   \n",
      "15         1       3  10253   1114     3821     397               964   \n",
      "16         2       3   1020   8816    12121     134              4508   \n",
      "17         1       3   5876   6157     2933     839               370   \n",
      "18         2       3  18601   6327    10099    2205              2767   \n",
      "19         1       3   7780   2495     9464     669              2518   \n",
      "20         2       3  17546   4519     4602    1066              2259   \n",
      "21         1       3   5567    871     2010    3383               375   \n",
      "22         1       3  31276   1917     4469    9408              2381   \n",
      "23         2       3  26373  36423    22019    5154              4337   \n",
      "24         2       3  22647   9776    13792    2915              4482   \n",
      "25         2       3  16165   4230     7595     201              4003   \n",
      "26         1       3   9898    961     2861    3151               242   \n",
      "27         1       3  14276    803     3045     485               100   \n",
      "28         2       3   4113  20484    25957    1158              8604   \n",
      "29         1       3  43088   2100     2609    1200              1107   \n",
      "..       ...     ...    ...    ...      ...     ...               ...   \n",
      "410        1       3   6633   2096     4563    1389              1860   \n",
      "411        1       3   2126   3289     3281    1535               235   \n",
      "412        1       3     97   3605    12400      98              2970   \n",
      "413        1       3   4983   4859     6633   17866               912   \n",
      "414        1       3   5969   1990     3417    5679              1135   \n",
      "415        2       3   7842   6046     8552    1691              3540   \n",
      "416        2       3   4389  10940    10908     848              6728   \n",
      "417        1       3   5065   5499    11055     364              3485   \n",
      "418        2       3    660   8494    18622     133              6740   \n",
      "419        1       3   8861   3783     2223     633              1580   \n",
      "420        1       3   4456   5266    13227      25              6818   \n",
      "421        2       3  17063   4847     9053    1031              3415   \n",
      "422        1       3  26400   1377     4172     830               948   \n",
      "423        2       3  17565   3686     4657    1059              1803   \n",
      "424        2       3  16980   2884    12232     874              3213   \n",
      "425        1       3  11243   2408     2593   15348               108   \n",
      "426        1       3  13134   9347    14316    3141              5079   \n",
      "427        1       3  31012  16687     5429   15082               439   \n",
      "428        1       3   3047   5970     4910    2198               850   \n",
      "429        1       3   8607   1750     3580      47                84   \n",
      "430        1       3   3097   4230    16483     575               241   \n",
      "431        1       3   8533   5506     5160   13486              1377   \n",
      "432        1       3  21117   1162     4754     269              1328   \n",
      "433        1       3   1982   3218     1493    1541               356   \n",
      "434        1       3  16731   3922     7994     688              2371   \n",
      "435        1       3  29703  12051    16027   13135               182   \n",
      "436        1       3  39228   1431      764    4510                93   \n",
      "437        2       3  14531  15488    30243     437             14841   \n",
      "438        1       3  10290   1981     2232    1038               168   \n",
      "439        1       3   2787   1698     2510      65               477   \n",
      "\n",
      "     Delicassen  \n",
      "0          1338  \n",
      "1          1776  \n",
      "2          7844  \n",
      "3          1788  \n",
      "4          5185  \n",
      "5          1451  \n",
      "6           545  \n",
      "7          2566  \n",
      "8           750  \n",
      "9          2098  \n",
      "10         1744  \n",
      "11          497  \n",
      "12         2931  \n",
      "13          602  \n",
      "14         2168  \n",
      "15          412  \n",
      "16         1080  \n",
      "17         4478  \n",
      "18         3181  \n",
      "19          501  \n",
      "20         2124  \n",
      "21          569  \n",
      "22         4334  \n",
      "23        16523  \n",
      "24         5778  \n",
      "25           57  \n",
      "26          833  \n",
      "27          518  \n",
      "28         5206  \n",
      "29          823  \n",
      "..          ...  \n",
      "410        1892  \n",
      "411        4365  \n",
      "412          62  \n",
      "413        2435  \n",
      "414         290  \n",
      "415        1874  \n",
      "416         993  \n",
      "417        1063  \n",
      "418         776  \n",
      "419        1521  \n",
      "420        1393  \n",
      "421        1784  \n",
      "422        1218  \n",
      "423         668  \n",
      "424         249  \n",
      "425        1886  \n",
      "426        1894  \n",
      "427        1163  \n",
      "428         317  \n",
      "429        2501  \n",
      "430        2080  \n",
      "431        1498  \n",
      "432         395  \n",
      "433        1449  \n",
      "434         838  \n",
      "435        2204  \n",
      "436        2346  \n",
      "437        1867  \n",
      "438        2125  \n",
      "439          52  \n",
      "\n",
      "[440 rows x 8 columns]\n",
      "Channel             0\n",
      "Region              0\n",
      "Fresh               0\n",
      "Milk                0\n",
      "Grocery             0\n",
      "Frozen              0\n",
      "Detergents_Paper    0\n",
      "Delicassen          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Datasets/Wholesale customers data.csv')\n",
    "print(df)\n",
    "print(df.isnull().sum()) # Shows that there are no NaNs in the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As instructed, get rid of the `Channel` and `Region` columns, since you'll be investigating as if this were a single location wholesaler, rather than a national / international one. Leaving these fields in here would cause KMeans to examine and give weight to them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(labels = ['Channel', 'Region'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before unitizing / standardizing / normalizing your data in preparation for K-Means, it's a good idea to get a quick peek at it. You can do this using the `.describe()` method, or even by using the built-in pandas `df.plot.hist()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Fresh          Milk       Grocery        Frozen  \\\n",
      "count     440.000000    440.000000    440.000000    440.000000   \n",
      "mean    12000.297727   5796.265909   7951.277273   3071.931818   \n",
      "std     12647.328865   7380.377175   9503.162829   4854.673333   \n",
      "min         3.000000     55.000000      3.000000     25.000000   \n",
      "25%      3127.750000   1533.000000   2153.000000    742.250000   \n",
      "50%      8504.000000   3627.000000   4755.500000   1526.000000   \n",
      "75%     16933.750000   7190.250000  10655.750000   3554.250000   \n",
      "max    112151.000000  73498.000000  92780.000000  60869.000000   \n",
      "\n",
      "       Detergents_Paper    Delicassen  \n",
      "count        440.000000    440.000000  \n",
      "mean        2881.493182   1524.870455  \n",
      "std         4767.854448   2820.105937  \n",
      "min            3.000000      3.000000  \n",
      "25%          256.750000    408.250000  \n",
      "50%          816.500000    965.500000  \n",
      "75%         3922.000000   1820.250000  \n",
      "max        40827.000000  47943.000000  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22c6b9c2320>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.describe())\n",
    "df.plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having checked out your data, you may have noticed there's a pretty big gap between the top customers in each feature category and the rest. Some feature scaling algorithms won't get rid of outliers for you, so it's a good idea to handle that manually---particularly if your goal is NOT to determine the top customers. \n",
    "\n",
    "After all, you can do that with a simple Pandas `.sort_values()` and not a machine learning clustering algorithm. From a business perspective, you're probably more interested in clustering your +/- 2 standard deviation customers, rather than the top and bottom customers.\n",
    "\n",
    "Remove top 5 and bottom 5 samples for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = {}\n",
    "for col in df.columns:\n",
    "    # Bottom 5\n",
    "    sort = df.sort_values(by=col, ascending=True)\n",
    "    if len(sort) > 5: sort=sort[:5]\n",
    "    for index in sort.index: drop[index] = True # Just store the index once\n",
    "\n",
    "    # Top 5\n",
    "    sort = df.sort_values(by=col, ascending=False)\n",
    "    if len(sort) > 5: sort=sort[:5]\n",
    "    for index in sort.index: drop[index] = True # Just store the index once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows by index. We do this all at once in case there is a collision. This way, we don't end up dropping more rows than we have to, if there is a single row that satisfies the drop for multiple columns. Since there are 6 rows, if we end up dropping < 5*6*2 = 60 rows, that means there indeed were collisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 42 Outliers...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10996.231156</td>\n",
       "      <td>5144.090452</td>\n",
       "      <td>7091.711055</td>\n",
       "      <td>2639.721106</td>\n",
       "      <td>2562.974874</td>\n",
       "      <td>1278.736181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9933.042596</td>\n",
       "      <td>5057.406574</td>\n",
       "      <td>6923.019293</td>\n",
       "      <td>2974.246906</td>\n",
       "      <td>3608.176776</td>\n",
       "      <td>1220.745297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3324.500000</td>\n",
       "      <td>1571.250000</td>\n",
       "      <td>2155.500000</td>\n",
       "      <td>749.750000</td>\n",
       "      <td>273.250000</td>\n",
       "      <td>409.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8257.500000</td>\n",
       "      <td>3607.500000</td>\n",
       "      <td>4573.000000</td>\n",
       "      <td>1526.000000</td>\n",
       "      <td>812.000000</td>\n",
       "      <td>946.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15828.500000</td>\n",
       "      <td>6953.250000</td>\n",
       "      <td>9922.250000</td>\n",
       "      <td>3370.250000</td>\n",
       "      <td>3841.500000</td>\n",
       "      <td>1752.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>53205.000000</td>\n",
       "      <td>29892.000000</td>\n",
       "      <td>39694.000000</td>\n",
       "      <td>17866.000000</td>\n",
       "      <td>19410.000000</td>\n",
       "      <td>7844.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Fresh          Milk       Grocery        Frozen  \\\n",
       "count    398.000000    398.000000    398.000000    398.000000   \n",
       "mean   10996.231156   5144.090452   7091.711055   2639.721106   \n",
       "std     9933.042596   5057.406574   6923.019293   2974.246906   \n",
       "min       37.000000    258.000000    314.000000     47.000000   \n",
       "25%     3324.500000   1571.250000   2155.500000    749.750000   \n",
       "50%     8257.500000   3607.500000   4573.000000   1526.000000   \n",
       "75%    15828.500000   6953.250000   9922.250000   3370.250000   \n",
       "max    53205.000000  29892.000000  39694.000000  17866.000000   \n",
       "\n",
       "       Detergents_Paper   Delicassen  \n",
       "count        398.000000   398.000000  \n",
       "mean        2562.974874  1278.736181  \n",
       "std         3608.176776  1220.745297  \n",
       "min           10.000000    11.000000  \n",
       "25%          273.250000   409.500000  \n",
       "50%          812.000000   946.500000  \n",
       "75%         3841.500000  1752.250000  \n",
       "max        19410.000000  7844.000000  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dropping {0} Outliers...\".format(len(drop)))\n",
    "df.drop(inplace=True, labels=drop.keys(), axis=0)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are you interested in?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Depending on what you're interested in, you might take a different approach to normalizing/standardizing your data.\n",
    " \n",
    "You should note that all columns left in the dataset are of the same unit. You might ask yourself, do I even need to normalize / standardize the data? The answer depends on what you're trying to accomplish. For instance, although all the units are the same (generic money unit), the price per item in your store isn't. There may be some cheap items and some expensive one. If your goal is to find out what items people tend to buy together but you didn't  \"unitize\" properly before running kMeans, the contribution of the lesser priced item would be dwarfed by the more expensive item. This is an issue of scale.\n",
    "\n",
    "For a great overview on a few of the normalization methods supported in SKLearn, please check out: https://stackoverflow.com/questions/30918781/right-function-for-normalizing-input-of-sklearn-svm\n",
    "\n",
    "Suffice to say, at the end of the day, you're going to have to know what question you want answered and what data you have available in order to select the best method for your purpose. Luckily, SKLearn's interfaces are easy to switch out so in the mean time, you can experiment with all of them and see how they alter your results.\n",
    "\n",
    "5-sec summary before you dive deeper online:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say your user spend a LOT. Normalization divides each item by the average overall amount of spending. Stated differently, your new feature is = the contribution of overall spending going into that particular item: \\$spent on feature / \\$overall spent by sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What % in the overall range of $spent by all users on THIS particular feature is the current sample's feature at? When you're dealing with all the same units, this will produce a near face-value amount. Be careful though: if you have even a single outlier, it can cause all your data to get squashed up in lower percentages.\n",
    "\n",
    "Imagine your buyers usually spend \\$100 on wholesale milk, but today only spent \\$20. This is the relationship you're trying to capture with MinMax. NOTE: MinMax doesn't standardize (std. dev.); it only normalizes / unitizes your feature, in the mathematical sense. MinMax can be used as an alternative to zero mean, unit variance scaling. [(sampleFeatureValue-min) / (max-min)] * (max-min) + min Where min and max are for the overall feature values for all samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to The Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un-comment just ***ONE*** of lines at a time and see how alters your results. Pay attention to the direction of the arrows, as well as their LENGTHS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T = preprocessing.StandardScaler().fit_transform(df)\n",
    "#T = preprocessing.MinMaxScaler().fit_transform(df)\n",
    "#T = preprocessing.MaxAbsScaler().fit_transform(df)\n",
    "#T = preprocessing.Normalizer().fit_transform(df)\n",
    "T = df # No Change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Sometimes people perform PCA before doing KMeans, so that KMeans only operates on the most meaningful features. In our case, there are so few features that doing PCA ahead of time isn't really necessary, and you can do KMeans in feature space. But keep in mind you have the option to transform your data to bring down its dimensionality. If you take that route, then your Clusters will already be in PCA-transformed feature space, and you won't have to project them again for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-842a209bccf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mn_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcentroids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-42-114f43587966>\u001b[0m in \u001b[0;36mdoKMeans\u001b[1;34m(data, num_clusters)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clusters' is not defined"
     ]
    }
   ],
   "source": [
    "# Do KMeans\n",
    "\n",
    "n_clusters = 3\n",
    "centroids, labels = doKMeans(T, n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out your centroids. They're currently in feature-space, which is good. Print them out before you transform them into PCA space for viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've clustered our KMeans, let's do PCA, using it as a tool to visualize the results. Project the centroids as well as the samples into the new 2D feature space for visualization purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_pca = doPCA(T)\n",
    "T = display_pca.transform(T)\n",
    "CC = display_pca.transform(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize all the samples. Give them the color of their cluster label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "if PLOT_TYPE_TEXT:\n",
    "    # Plot the index of the sample, so you can further investigate it in your dset\n",
    "    for i in range(len(T)): ax.text(T[i,0], T[i,1], df.index[i], color=c[labels[i]], alpha=0.75, zorder=600000)\n",
    "    ax.set_xlim(min(T[:,0])*1.2, max(T[:,0])*1.2)\n",
    "    ax.set_ylim(min(T[:,1])*1.2, max(T[:,1])*1.2)\n",
    "else:\n",
    "    # Plot a regular scatter plot\n",
    "    sample_colors = [ c[labels[i]] for i in range(len(T)) ]\n",
    "    ax.scatter(T[:, 0], T[:, 1], c=sample_colors, marker='o', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Centroids as X's, and label them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.scatter(CC[:, 0], CC[:, 1], marker='x', s=169, linewidths=3, zorder=1000, c=c)\n",
    "for i in range(len(centroids)):\n",
    "    ax.text(CC[i, 0], CC[i, 1], str(i), zorder=500010, fontsize=18, color=c[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display feature vectors for investigation:\n",
    "if PLOT_VECTORS:\n",
    "    drawVectors(T, display_pca.components_, df.columns, plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the cluster label back into the dataframe and display it:\n",
    "df['label'] = pd.Series(labels, index=df.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "58px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
